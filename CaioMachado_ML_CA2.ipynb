{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d03c262",
   "metadata": {
    "id": "3d03c262"
   },
   "outputs": [],
   "source": [
    "#Importing all libraries\n",
    "\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "# Scikit-learn libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# TensorFlow and Keras libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import backend as k\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# NLTK and TextBlob libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import download_corpora, TextBlob\n",
    "\n",
    "# Imbalanced-learn library\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ecf0b0",
   "metadata": {
    "id": "99ecf0b0",
    "outputId": "4b19d5d0-3c53-4d9f-e417-d6f63604caf4"
   },
   "outputs": [],
   "source": [
    "# Download NLTK corpora\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "download_corpora.download_all()\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12f2974",
   "metadata": {
    "id": "c12f2974"
   },
   "source": [
    "# Neural Network - Part I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffda2606",
   "metadata": {
    "id": "ffda2606"
   },
   "outputs": [],
   "source": [
    "# importing dataset\n",
    "filename = 'BankRecords.csv'\n",
    "data = pd.read_csv(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iUb4uYyqPNJw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1716950924093,
     "user": {
      "displayName": "Caio Oliveira",
      "userId": "01921506838875341927"
     },
     "user_tz": -60
    },
    "id": "iUb4uYyqPNJw",
    "outputId": "297594c3-1394-4bc4-baf9-48e9713be823",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f5eb54",
   "metadata": {
    "id": "b0f5eb54"
   },
   "source": [
    "## EDA - Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fc0337",
   "metadata": {
    "id": "96fc0337"
   },
   "source": [
    "### Numerical Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12017cc8",
   "metadata": {
    "id": "12017cc8"
   },
   "source": [
    "When first looking at the numerical features and its attributes, it is noticed that the feature \"Experience(Years)\" contain negative values that might cause problems in further applications.\n",
    "It was identified that the features 'ID' and 'Sort Code' are not important for this application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b14560d",
   "metadata": {
    "id": "5b14560d",
    "outputId": "3a50871b-2d04-436b-cff8-391fdb35fa7e"
   },
   "outputs": [],
   "source": [
    "# Checking numerical values\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d8f5c6",
   "metadata": {
    "id": "54d8f5c6",
    "outputId": "d46058da-1bed-4000-af80-ba98331a8325"
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f63362e",
   "metadata": {
    "id": "5f63362e",
    "outputId": "9ab957a8-c424-4c74-f70f-6f3ebfe38db0"
   },
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7476dbb7",
   "metadata": {
    "id": "7476dbb7"
   },
   "source": [
    "### Categorical Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9de9c0",
   "metadata": {
    "id": "ed9de9c0"
   },
   "source": [
    "It is important to check que unique values to check for mispelling. '.unique()' allow us to visualize it. '.value_counts()' let us count each value on each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff24634",
   "metadata": {
    "id": "8ff24634",
    "outputId": "ed563a61-939f-4cd4-9397-d3289b0a921e"
   },
   "outputs": [],
   "source": [
    "print(data['Education'].value_counts())\n",
    "print(data['Education'].unique())\n",
    "\n",
    "print(\"\\n\")\n",
    "print(data['Personal Loan'].value_counts())\n",
    "print(data['Personal Loan'].unique())\n",
    "\n",
    "print(\"\\n\")\n",
    "print(data['Securities Account'].value_counts())\n",
    "print(data['Securities Account'].unique())\n",
    "\n",
    "print(\"\\n\")\n",
    "print(data['CD Account'].value_counts())\n",
    "print(data['CD Account'].unique())\n",
    "\n",
    "print(\"\\n\")\n",
    "print(data['Online Banking'].value_counts())\n",
    "print(data['Online Banking'].unique())\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "print(data['CreditCard'].value_counts())\n",
    "print(data['CreditCard'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820ad3a0",
   "metadata": {
    "id": "820ad3a0"
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44379e40",
   "metadata": {
    "id": "44379e40"
   },
   "source": [
    "To prepare the data for a model application demand understanding of what will and won't be relevant, and may or may not affect the model accuracy.\n",
    "After describing the numerical values on the EDA section, negative values were found on 'Experience(Years)' and will be removed as it might affect the model accuracy.\n",
    "Was identified that 'ID' and 'Sort Code' are irrelevant for our model application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acf4b50",
   "metadata": {
    "id": "0acf4b50"
   },
   "outputs": [],
   "source": [
    "# Drop the 'ID' and 'Sort Code' columns and rename Income column for convenience\n",
    "data = data.drop(['ID', 'Sort Code'], axis=1)\n",
    "data = data.rename(columns={'Income(Thousands\\'s)': 'Income'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcb5770",
   "metadata": {
    "id": "9bcb5770"
   },
   "source": [
    "We have addressed that only the 'Experience(Years)' feature contain negative values, now we only need to count and remove the entire row from our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1a98e2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1716950924095,
     "user": {
      "displayName": "Caio Oliveira",
      "userId": "01921506838875341927"
     },
     "user_tz": -60
    },
    "id": "ac1a98e2",
    "outputId": "b729571c-bd06-497f-ab3f-73163efcf6ef"
   },
   "outputs": [],
   "source": [
    "# Count the number of negative observations\n",
    "negative_value = (data['Experience(Years)'] < 0).sum().sum()\n",
    "print(f\"Number of negative values on 'Age': {negative_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20044eb",
   "metadata": {
    "id": "c20044eb",
    "outputId": "a650e160-f4e0-468e-984f-7012a331b0a6"
   },
   "outputs": [],
   "source": [
    "# Selecting rows with negative values in 'Experience(Years)' column\n",
    "negative_rows = data[data['Experience(Years)'] < 0]\n",
    "# Displaying the selected rows\n",
    "print(\"Rows with negative values in 'Experience(Years)' column:\")\n",
    "negative_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c1ad9a",
   "metadata": {
    "id": "a6c1ad9a"
   },
   "outputs": [],
   "source": [
    "# Remove rows with negative values in 'Experience(Years)' column\n",
    "data = data[data['Experience(Years)'] >= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123bef8b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1716950924473,
     "user": {
      "displayName": "Caio Oliveira",
      "userId": "01921506838875341927"
     },
     "user_tz": -60
    },
    "id": "123bef8b",
    "outputId": "4c56e189-53fb-435e-c719-24f5424c0b90"
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a83ff0",
   "metadata": {
    "id": "06a83ff0"
   },
   "source": [
    "### Processing features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19ecb0e",
   "metadata": {
    "id": "b19ecb0e"
   },
   "source": [
    "### StandardScaler () & OneHotEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a53a07",
   "metadata": {
    "id": "08a53a07"
   },
   "source": [
    "Processing the features play a fundamental role in our model. StandardScaler and OneHoteEncoder work together but have diffirent functions.\n",
    "StandardScaler is applied to numerical values that have different range ensuring the mean is equal to 0 and the standard deviation is equal to 1. Another option would be MinMaxScaler, which was tested previously and got worse accuracy.\n",
    "OneHotEncode simply convert categorical variable into numerical, creating a binary column for each category indicating the presence or absence of that category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_eVCvRvHItNb",
   "metadata": {
    "id": "_eVCvRvHItNb"
   },
   "outputs": [],
   "source": [
    "# Separate the target variable from the processing\n",
    "income = data.pop('Income')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d387112",
   "metadata": {
    "id": "5d387112"
   },
   "outputs": [],
   "source": [
    "# List of categorical and numerical features\n",
    "categorical_features = ['Education', 'Personal Loan', 'Securities Account', 'CD Account', 'Online Banking', 'CreditCard']\n",
    "numerical_features = ['Age', 'Experience(Years)', 'Family', 'Credit Score', 'Mortgage(Thousands\\'s)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0319483f",
   "metadata": {
    "id": "0319483f"
   },
   "outputs": [],
   "source": [
    "# Creating transformers for numerical and categorical data\n",
    "numerical_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(drop='first')  # Use drop='first' to avoid multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0afd2ab",
   "metadata": {
    "id": "e0afd2ab"
   },
   "outputs": [],
   "source": [
    "# Combine transformers into a preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070e2b08",
   "metadata": {
    "id": "070e2b08"
   },
   "outputs": [],
   "source": [
    "# Apply the transformations\n",
    "data_preprocessed = preprocessor.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024b121e",
   "metadata": {
    "id": "024b121e"
   },
   "outputs": [],
   "source": [
    "# Convert the preprocessed data to a DataFrame for better readability\n",
    "# Getting feature names for the new columns created by OneHotEncoder\n",
    "encoded_columns = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)\n",
    "all_columns = numerical_features + list(encoded_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f99297",
   "metadata": {
    "id": "53f99297"
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame\n",
    "data_model = pd.DataFrame(data_preprocessed, columns=all_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XVZB8LesIQNq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 291,
     "status": "ok",
     "timestamp": 1716950924989,
     "user": {
      "displayName": "Caio Oliveira",
      "userId": "01921506838875341927"
     },
     "user_tz": -60
    },
    "id": "XVZB8LesIQNq",
    "outputId": "75333d2c-959c-42be-c9d6-0ed22fd8d39f"
   },
   "outputs": [],
   "source": [
    "data_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ITINoTq8IfVZ",
   "metadata": {
    "id": "ITINoTq8IfVZ"
   },
   "outputs": [],
   "source": [
    "# Add target variable back to data\n",
    "data_model['Income'] = income.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8606a6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 340,
     "status": "ok",
     "timestamp": 1716950925326,
     "user": {
      "displayName": "Caio Oliveira",
      "userId": "01921506838875341927"
     },
     "user_tz": -60
    },
    "id": "ab8606a6",
    "outputId": "9d93b204-f2b0-488a-aca9-2605ab69fa83"
   },
   "outputs": [],
   "source": [
    "data_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229dd223",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1716950925327,
     "user": {
      "displayName": "Caio Oliveira",
      "userId": "01921506838875341927"
     },
     "user_tz": -60
    },
    "id": "229dd223",
    "outputId": "5e4a4732-779e-4a90-c056-8c5c0b95eeb5"
   },
   "outputs": [],
   "source": [
    "data_model.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c6d942",
   "metadata": {
    "id": "b6c6d942"
   },
   "source": [
    "# Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8248163c",
   "metadata": {
    "id": "8248163c"
   },
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "X = data_model.drop('Income', axis=1)\n",
    "y = data_model['Income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eae9825",
   "metadata": {
    "id": "4eae9825"
   },
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train , y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff1d606",
   "metadata": {
    "id": "3ff1d606"
   },
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c40715c",
   "metadata": {
    "id": "6c40715c"
   },
   "source": [
    "Neural Network is a subset of machine learning and is known as the backbone of deep learning algorithms. The neural part of the model mimics the neurons in the brain. Neural Network, also called Artificial Neural Network, is made up of node layers, an input layer, hidden layer (one or more), and an output layer (Raschka and Mirjalili, 2019). Each of these nodes are connected to the next one and has a weight and threshold value on them. In the image below we can see a Neural Network of 5 layers. An Input layer, three hidden layers, and an output layer (Grus, 2015).\n",
    "\n",
    "\n",
    "Neural Network support different ways of train models. The most known are feed-forward and back-propagation. Feed-forward simply goes in one direction, from input to output. On the other hand, backpropagation means going in the opposite direction. It takes the total error and goes back to each neurons distributing accordingly minimizing the error and adjusting the weights and biases of the network.  (IBM, 2024)\n",
    "\n",
    "In our model, we used 'ReLU' for activation function because it is the most used in regression application and dealt better with the complexity of the data (Oppermann, 2021). After running many trainings checking for the best parameters, we got the best results using only one hidden layer with '64 units'. The output layer must have the units equal the expected value of the prediction which is one feature (income) in our case. The activation function used in the output layer in most regression models is 'linear' which is suitable for predicting continuous values. The optimizer uses 'Adam' which is a stochastic gradient descent, and plays a important role in the model minimizing the loss function, such as Mean Squared Error (MSE), used in our model. MSE it is defined as the average of the squared differences between the predicted and actual values.\n",
    "\n",
    "So the model is built and ready to run. After exausting trainings, the best results was a score ranging 63% to 65%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6d2947",
   "metadata": {
    "executionInfo": {
     "elapsed": 360,
     "status": "ok",
     "timestamp": 1716950925683,
     "user": {
      "displayName": "Caio Oliveira",
      "userId": "01921506838875341927"
     },
     "user_tz": -60
    },
    "id": "eb6d2947",
    "outputId": "b8a6f076-270b-470a-9e5d-2d90aa94d232"
   },
   "outputs": [],
   "source": [
    "# Define the Keras Sequential model with tuned hyperparameters\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compile the model with a tuned optimizer\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2957a0bf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32362,
     "status": "ok",
     "timestamp": 1716950958043,
     "user": {
      "displayName": "Caio Oliveira",
      "userId": "01921506838875341927"
     },
     "user_tz": -60
    },
    "id": "2957a0bf",
    "outputId": "b9fd6da8-aa88-4d1a-a8f5-3cb3d1c1cbfc"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, callbacks=[early_stop], verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3f2b4d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 381,
     "status": "ok",
     "timestamp": 1716950958653,
     "user": {
      "displayName": "Caio Oliveira",
      "userId": "01921506838875341927"
     },
     "user_tz": -60
    },
    "id": "ea3f2b4d",
    "outputId": "0acd2b2d-d501-4ab2-f9bd-106bb3d888ed"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "y_pred_keras = model.predict(X_test)\n",
    "mse_keras = mean_squared_error(y_test, y_pred_keras)\n",
    "rmse_keras = np.sqrt(mse_keras)\n",
    "r2_keras = r2_score(y_test, y_pred_keras)\n",
    "\n",
    "print(f'Keras MSE: {mse_keras}')\n",
    "print(f'Keras RMSE: {rmse_keras}')\n",
    "print(f'Keras R²: {r2_keras}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aff3d0",
   "metadata": {
    "id": "42aff3d0"
   },
   "source": [
    "# Machine Learning Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337f728e",
   "metadata": {
    "id": "337f728e"
   },
   "source": [
    "In this assignment, we explored two very common used machine learning models: Linear Regression and Random Forest Regressor. Both models are from the Scikt-learn library and have distinct characteristics and are utilized for regression problems in order to predict a continuous target variable based on input features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b7ba7e",
   "metadata": {
    "id": "62b7ba7e"
   },
   "source": [
    "## Linear Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40221b37",
   "metadata": {
    "id": "40221b37"
   },
   "source": [
    "LinearRegression fits a linear model with coefficients w = (w1, …, wp) to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation (scikit-learn developers, 2019).\n",
    "\n",
    "Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables (Andriy Burkov, 2019). Its choice was a decision for modeling using a different approach in order to explorer the nuanses of the dataset when model building. The way Linear Regression model works is fitting a linear equation to the observed data, minimizing the sum of squared residuals between the observed and predicted values.\n",
    "\n",
    "\n",
    "The model has a simple application method and do not demand a lot of tune for its hyperparameters.\n",
    "\n",
    "After applying the model, the result was a R² score = 0.54 (54%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3ff3aa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1716950958653,
     "user": {
      "displayName": "Caio Oliveira",
      "userId": "01921506838875341927"
     },
     "user_tz": -60
    },
    "id": "ab3ff3aa",
    "outputId": "6a61a5e3-1cb9-40fa-ac7f-b51726cf0d56"
   },
   "outputs": [],
   "source": [
    "# Train a Linear Regression model\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d29c027",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1716950958654,
     "user": {
      "displayName": "Caio Oliveira",
      "userId": "01921506838875341927"
     },
     "user_tz": -60
    },
    "id": "2d29c027",
    "outputId": "ed57d24c-54dc-4820-abbd-4dad2cf094e7"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "y_pred_linear = linear_model.predict(X_test)\n",
    "mse_linear = mean_squared_error(y_test, y_pred_linear)\n",
    "rmse_linear = np.sqrt(mse_linear)\n",
    "r2_linear = r2_score(y_test, y_pred_linear)\n",
    "\n",
    "print(f'Linear Regression MSE: {mse_linear}')\n",
    "print(f'Linear Regression RMSE: {rmse_linear}')\n",
    "print(f'Linear Regression R²: {r2_linear}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e443910",
   "metadata": {
    "id": "2e443910"
   },
   "source": [
    "## Random Forest Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77811e1",
   "metadata": {
    "id": "b77811e1"
   },
   "source": [
    "Random Forest Regressor, on the other hand, is an ensemble learning method that leverages the power of multiple decision trees to improve predictive performance and robustness. The random forest algorithm constructs a multitude of decision trees during training and outputs the mean prediction of the individual trees for regression tasks. This approach helps to mitigate overfitting, enhance model accuracy, and handle datasets with higher dimensionality (IBM, 2023). Random Forest Regressor is particularly effective in capturing complex interactions and non-linear relationships between features, making it a versatile and powerful tool for various regression problems.\n",
    "\n",
    "Random Forests models make use of what is called bagging, which consists of creating many “copies” of the training data (each copy is slightly different from another) and then applying the weak learner to each copy to obtain multiple weak models and then combine them. (Hastie, Tibshirani and Friedman, 2009)\n",
    "\n",
    "Our model performed well capturing the complexity of the data regardless of the fact that it is not a large dataset. The result we have got applying the model was R² score = 0.79 (79%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6eb6a0",
   "metadata": {
    "id": "6b6eb6a0"
   },
   "outputs": [],
   "source": [
    "# Tune the parameters for make_regression\n",
    "X, y = make_regression(\n",
    "    n_samples=1000,        # Number of samples\n",
    "    n_features=12,         # Number of features\n",
    "    n_informative=6,       # Number of informative features\n",
    "    noise=0.2,             # Standard deviation of Gaussian noise\n",
    "    bias=0.5,              # Bias term in the underlying linear model\n",
    "    random_state=42        # Seed for reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115f0db6",
   "metadata": {
    "id": "115f0db6"
   },
   "outputs": [],
   "source": [
    "# Create a Random Forest Regressor\n",
    "model_RF = RandomForestRegressor(n_estimators=100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba97ce3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "executionInfo": {
     "elapsed": 2183,
     "status": "ok",
     "timestamp": 1716950997063,
     "user": {
      "displayName": "Caio Oliveira",
      "userId": "01921506838875341927"
     },
     "user_tz": -60
    },
    "id": "3ba97ce3",
    "outputId": "6d48e556-f1a6-4eed-c7fd-30ef320032a4"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model_RF.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3372eb29",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1716950997063,
     "user": {
      "displayName": "Caio Oliveira",
      "userId": "01921506838875341927"
     },
     "user_tz": -60
    },
    "id": "3372eb29",
    "outputId": "750ad454-2372-496e-9989-a08207c0f788"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "y_pred_RF = model_RF.predict(X_test)\n",
    "mse_RF = mean_squared_error(y_test, y_pred_RF)\n",
    "rmse_RF = np.sqrt(mse_RF)\n",
    "r2_rf = r2_score(y_test, y_pred_RF)\n",
    "\n",
    "print(f'Random Forest Regressor MSE: {mse_RF}')\n",
    "print(f'Random Forest Regressor RMSE: {rmse_RF}')\n",
    "print(f'Random Forest Regressor R²: {r2_rf}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3509dec2",
   "metadata": {
    "id": "3509dec2"
   },
   "source": [
    "# Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e315b1fc",
   "metadata": {
    "id": "e315b1fc"
   },
   "source": [
    "Based on our models after trained and tested, the Random Forest Regressor performed the best compared to Keras and Linear Regression. Despite of Keras being a Neural Network model and deals much better with complex data in general, the fact that our dataset has not a large number of obsearvation (5000 in total) may have affected the results not capturing the underlying patterns effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a25f8a",
   "metadata": {
    "id": "68a25f8a",
    "outputId": "46a4dc8e-1ca3-4810-f13c-a25849d4cf1e"
   },
   "outputs": [],
   "source": [
    "print(f'Keras MSE: {mse_keras:.2f}')\n",
    "print(f'Keras R²: {r2_keras*100:.2f}%')\n",
    "\n",
    "\n",
    "print(f'\\nLinear Regression MSE: {mse_linear:.2f}')\n",
    "print(f'Linear Regression R²: {r2_linear*100:.2f}%')\n",
    "\n",
    "\n",
    "print(f'\\nRandom Forest Regressor MSE: {mse_RF:.2f}')\n",
    "print(f'Random Forest Regressor R²: {r2_rf*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3973940",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 358,
     "status": "ok",
     "timestamp": 1716950997419,
     "user": {
      "displayName": "Caio Oliveira",
      "userId": "01921506838875341927"
     },
     "user_tz": -60
    },
    "id": "f3973940",
    "outputId": "c9a91a2e-7e24-4fec-e7f4-8e34e57b9ea1"
   },
   "outputs": [],
   "source": [
    "# New customer details\n",
    "new_customer = {\n",
    "    'Age': 30,\n",
    "    'Experience(Years)': 5,\n",
    "    'Sort Code': 92011,\n",
    "    'Family': 2,\n",
    "    'Credit Score': 1.2,\n",
    "    'Mortgage(Thousands\\'s)': 20,\n",
    "    'Education': 'Degree',\n",
    "    'Personal Loan': 'No',\n",
    "    'Securities Account': 'Yes',\n",
    "    'CD Account': 'No',\n",
    "    'Online Banking': 'Yes',\n",
    "    'CreditCard': 'No'\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "new_customer_df = pd.DataFrame([new_customer])\n",
    "\n",
    "# Apply the same preprocessing to the new customer data\n",
    "new_customer_preprocessed = preprocessor.transform(new_customer_df)\n",
    "\n",
    "\n",
    "# Predict income using the Random Forest model\n",
    "new_customer_income_RF = model_RF.predict(new_customer_preprocessed)\n",
    "\n",
    "# Print the results\n",
    "print(\"Predicted income (in thousands) using Random Forest model:\", new_customer_income_RF[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a3a226",
   "metadata": {
    "id": "47a3a226"
   },
   "source": [
    "# Semantic Analysis - Part II"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a35b318",
   "metadata": {
    "id": "d246344d"
   },
   "source": [
    "Semantic Analysis is a Natural Language Processing approach. It refers to analyzing the relationship of words, sentences and entire text within a give context (Nitin Indurkhya and Damerau, 2010). Using Neural Network model goes beyond analyzing only the meaning and grammar of words. It requires a set of process to extract insights and analyse the sentiment on each sentence. \n",
    "\n",
    "This approach can be useful when apply to businesses that deal with customer satisfaction extracting information from unstructured data. Booking.com was used to retrieve information from a Hotel and gather guests reviews from 2021 to 2023. \n",
    "\n",
    "The use of TensorFlow and Keras was the choice for applying the model after going through a set of tasks aiming the best model performance accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c92cd02",
   "metadata": {
    "id": "5c92cd02"
   },
   "outputs": [],
   "source": [
    "file_path = 'reviews.csv'\n",
    "data_review = pd.read_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345e1e7f",
   "metadata": {
    "id": "345e1e7f"
   },
   "outputs": [],
   "source": [
    "data_review.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb546fb7",
   "metadata": {
    "id": "cb546fb7"
   },
   "outputs": [],
   "source": [
    "data_review = data_review.drop(['Review date', 'Guest name', 'Reservation Number','Review title','Staff','Cleanliness','Location','Facilities','Comfort','Value for money','Property reply' ], axis=1)\n",
    "data_review.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0591f2af",
   "metadata": {
    "id": "0591f2af"
   },
   "source": [
    "### Applying Lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b476182",
   "metadata": {
    "id": "3b476182"
   },
   "source": [
    "In the context of tokenization, another useful technique is word stemming, which is the process of\n",
    "transforming a word into its root form. It allows us to map related words to the same stem (Raschka and Mirjalili, 2019).\n",
    "The choice for WordNetLemmatizer() is the fact that it works better with different languages, which is very important in our case as we are dealing with hotel reviews and contain reviews in different languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f048c0",
   "metadata": {
    "id": "d3f048c0"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    words = text.split()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, wordnet.VERB) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3485f2c2",
   "metadata": {
    "id": "3485f2c2"
   },
   "outputs": [],
   "source": [
    "# Combine the 'Positive review' and 'Negative review' columns into a single 'Review' column\n",
    "data_review['Review'] = data_review['Positive review'].fillna('') + ' ' + data_review['Negative review'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e973cb37",
   "metadata": {
    "id": "e973cb37"
   },
   "outputs": [],
   "source": [
    "# Apply lemmatization to the 'Review' column\n",
    "data_review['Review'] = data_review['Review'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fd7c84",
   "metadata": {
    "id": "58fd7c84",
    "outputId": "6491aee4-2ae7-46db-98bb-af264dd022e4"
   },
   "outputs": [],
   "source": [
    "data_review.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752dacd4",
   "metadata": {
    "id": "752dacd4"
   },
   "outputs": [],
   "source": [
    "data_review = data_review.drop(['Positive review','Negative review' ], axis=1)\n",
    "data_review.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c1d76c",
   "metadata": {
    "id": "42c1d76c",
    "outputId": "05d32f23-5215-4af9-d704-e14a112ca9e6"
   },
   "outputs": [],
   "source": [
    "data_review.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf73f7b",
   "metadata": {
    "id": "7cf73f7b"
   },
   "source": [
    "### Applying TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bca9379",
   "metadata": {},
   "source": [
    "TextBlob is an open-source library in Python for processing textual data. It is a powerful package that simplifies the complexity of contextual data and extracts in-depth information from the text (Analytics Vidhya, 2018). TextBlob can handle a lot of different tasks such as Tokenization, Word frequencies, Lemmatization, Spelling correction, and so on. In our sentiment analysis case, TextBlob is useful for determining whether the input textual data has a positive, negative, or neutral tone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441e39f3",
   "metadata": {
    "id": "441e39f3"
   },
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    analysis = TextBlob(text)\n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return 'Positive'\n",
    "    elif analysis.sentiment.polarity == 0:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Negative'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55131336",
   "metadata": {
    "id": "55131336"
   },
   "outputs": [],
   "source": [
    "# Apply sentiment analysis to 'Review' column\n",
    "data_review['Review Sentiment'] = data_review['Review'].apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f376bb",
   "metadata": {
    "id": "42f376bb",
    "outputId": "8963ffe5-f72c-47e8-b61b-aa426cc99fee"
   },
   "outputs": [],
   "source": [
    "data_review.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88d47e6",
   "metadata": {
    "id": "d88d47e6",
    "outputId": "d40a4fdf-208e-496f-f290-ef8ac24e79d7"
   },
   "outputs": [],
   "source": [
    "data_review['Review Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b627271",
   "metadata": {
    "id": "8b627271"
   },
   "source": [
    "### Visualizing Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b80446",
   "metadata": {
    "id": "d7b80446"
   },
   "outputs": [],
   "source": [
    "# Assuming data_review is your DataFrame\n",
    "data_review['Review score'] = data_review['Review score'].round().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce443be0",
   "metadata": {
    "id": "ce443be0",
    "outputId": "18ac113e-6d1b-471d-de87-107c67358580"
   },
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "sns.countplot(x='Review score',hue='Review score',data=data_review, palette='flare', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62173dca",
   "metadata": {
    "id": "62173dca",
    "outputId": "b3ed8d81-b5c4-43b3-99aa-0c9ed37777c7"
   },
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "sns.countplot(x='Review Sentiment',hue='Review Sentiment',data=data_review, palette='flare', legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5155f761",
   "metadata": {
    "id": "5155f761"
   },
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc80b30",
   "metadata": {
    "id": "ecc80b30"
   },
   "source": [
    "We will now remove all punctuation marks, lower all the letters, remove stopwords (e.g. the, and, is),and the Unicode emoji formats (e.g. Ã°ÂŸÂ‘Â, meaning \"thumbs up\") found in our dataset. To accomplish this task, we will use Python's regular expression (regex) library, re , as shown below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bac67b7",
   "metadata": {
    "id": "3bac67b7"
   },
   "outputs": [],
   "source": [
    "def get_text_processing(text):\n",
    "    # Remove unwanted characters and keep only letters and numbers\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s]', '', text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenize and remove stopwords\n",
    "    stpword = stopwords.words('english')\n",
    "    words = text.split()\n",
    "    return ' '.join([word for word in words if word not in stpword])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2877270c",
   "metadata": {
    "id": "2877270c"
   },
   "outputs": [],
   "source": [
    "data_review['Review'] = data_review['Review'].apply(get_text_processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07affaf",
   "metadata": {
    "id": "f07affaf",
    "outputId": "bff02699-56d5-4d1f-c9a2-12cee694a27f"
   },
   "outputs": [],
   "source": [
    "data_review.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79078c39",
   "metadata": {
    "id": "79078c39",
    "outputId": "20183c1d-503d-4fa0-a205-4703a4375f4f"
   },
   "outputs": [],
   "source": [
    "data_review.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c46bcd",
   "metadata": {
    "id": "05c46bcd"
   },
   "outputs": [],
   "source": [
    "data_model = data_review[['Review', 'Review Sentiment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341c8449",
   "metadata": {},
   "source": [
    "### One-Hot-Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc13920",
   "metadata": {},
   "source": [
    "The get_dummies function is a very simple way to encode the data. The get_dummies function automatically transforms all columns that are categorical. This method simply convert categorical variable into numerical, creating a binary column for each category indicating the presence or absence of that category (Müller and Guido, 2017)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f2cf3d",
   "metadata": {
    "id": "a7f2cf3d",
    "outputId": "a036f320-76c5-4176-b51e-43bc227e23ac"
   },
   "outputs": [],
   "source": [
    "one_hot = pd.get_dummies(data_model[\"Review Sentiment\"])\n",
    "data_model.drop(['Review Sentiment'],axis=1,inplace=True)\n",
    "data_model = pd.concat([data_model,one_hot],axis=1)\n",
    "data_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59d452a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0595a05",
   "metadata": {
    "id": "a0595a05"
   },
   "source": [
    "## Spliting Training and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4771ab34",
   "metadata": {
    "id": "4771ab34"
   },
   "outputs": [],
   "source": [
    "X = data_model['Review'].values\n",
    "y = data_model.drop('Review', axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6added1a",
   "metadata": {
    "id": "6added1a"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "U9NT7KnEhgSy",
   "metadata": {
    "id": "U9NT7KnEhgSy"
   },
   "source": [
    "## Tf-idf\n",
    "\n",
    "The term frequency–inverse document frequency (tf–idf) is a common method in text classification . The function of this method is to give high weight to any term that appears often in a particular document.\n",
    "\n",
    "There are two ways of applying the Tf-idf. Either applying CountVectorizer, which will produce a sparse matrix and use TfidfTransformer. Otherwise, TfidfVectorizer, which takes in the text data and does both the bag-of-words feature extraction and the tf–idf transformation. (Raschka and Mirjalili, 2019)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b744e8e",
   "metadata": {
    "id": "3b744e8e"
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer()\n",
    "X_train = vect.fit_transform(X_train)\n",
    "X_test = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dce054",
   "metadata": {
    "id": "75dce054"
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer()\n",
    "X_train = tfidf.fit_transform(X_train)\n",
    "X_test = tfidf.transform(X_test)\n",
    "#X_train = X_train.toarray()\n",
    "#X_test = X_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b8eac3",
   "metadata": {
    "id": "65b8eac3"
   },
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126276ea",
   "metadata": {
    "id": "126276ea",
    "outputId": "df4fb3e2-18f9-4ff1-ba04-2c01f389ea44"
   },
   "outputs": [],
   "source": [
    "# Verify the distribution\n",
    "print(f\"Original dataset shape: {Counter(np.argmax(y_train, axis=1))}\")\n",
    "print(f\"Resampled dataset shape: {Counter(np.argmax(y_resampled, axis=1))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed80a779",
   "metadata": {
    "id": "ed80a779"
   },
   "outputs": [],
   "source": [
    "# Combine the resampled data into a DataFrame (Note: X_resampled needs to be converted to a dense format)\n",
    "data_balanced = pd.DataFrame(X_resampled.toarray(), columns=vect.get_feature_names_out())\n",
    "data_balanced = pd.concat([data_balanced, pd.DataFrame(y_resampled, columns=one_hot.columns)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1133f0de",
   "metadata": {},
   "source": [
    "## Defining Keras Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e97b1e",
   "metadata": {},
   "source": [
    "We created a Sequential model (https://keras.io/models/sequential/) and added layers one at a time doing tests with as many combinations as we can until we are happy with our network architecture. The first thing to get right is to ensure the input layer has the right number of input features. This can be specified when creating the first layer with the input_dim argument and setting it to the shape of our dataset after all preparation. We ended up with three hidden layers (128, 64, and 32), and an output layer of 3, meaning we can get three possible solution (Positive, Negative and Neutral).\n",
    "\n",
    "We used activation function referred to as 'ReLU' on the three hidden layers and the 'softmax' function in the output layer. Softmax activation in the last layer is used to support multi-class classification, since here we have three class labels\n",
    "(which is why we have three neurons at the output layer)(Raschka and Mirjalili, 2019).\n",
    "\n",
    "We specified the loss function as 'categorical_crossentropy' to evaluate a set of weights, the optimizer was 'Adam', and the metric was 'accuracy'. An early stop function was used to avoid overfitting when training the model. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74b8143",
   "metadata": {
    "id": "f74b8143",
    "outputId": "47bdbd3c-64a0-4a97-f8ac-57da7e86e2cb"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=128, input_dim=X_resampled.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(units=64,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(units=32,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(units=3, activation='softmax'))\n",
    "\n",
    "opt=tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb47547",
   "metadata": {
    "id": "1fb47547",
    "outputId": "545bdcdc-5276-4c72-e158-933989b1a9d4"
   },
   "outputs": [],
   "source": [
    "# Train the model with a different batch size and more epochs\n",
    "history = model.fit(X_resampled, y_resampled, validation_data=(X_test, y_test), epochs=20, batch_size=256, verbose=1, callbacks=[early_stop])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac2ead1",
   "metadata": {
    "id": "bac2ead1",
    "outputId": "81006916-2df7-4da7-94f0-f5cb6148592e"
   },
   "outputs": [],
   "source": [
    "# Predict the labels for the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MUE_la0Al3Gk",
   "metadata": {
    "id": "MUE_la0Al3Gk"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "#### Confusion Matrix\n",
    "\n",
    "A confusion matrix is simply a square matrix that reports the counts of the true positive (TP), true negative (TN), false positive (FP), and false negative (FN) predictions of a classifier, which is represented by a matrix nxn with the rows being labeled with the classes and the columns labeled with the predicted classes (Provost and Fawcett, 2013). In this study, we will deal with a 3x3 Matrix, as we have 3 classes in our target value.\n",
    "\n",
    "#### Results\n",
    "\n",
    "After the model is fit, the model evalutation performed with an accuracy of 87%, which seems to be a good model. We can see by looking at the confusion matrix that most prediction are correctly predicted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dbcd0b",
   "metadata": {
    "id": "86dbcd0b",
    "outputId": "a8d1403e-841e-4385-c44f-ef59f6338862"
   },
   "outputs": [],
   "source": [
    "# Generate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=one_hot.columns, yticklabels=one_hot.columns)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_true, y_pred_classes, target_names=one_hot.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8e8a15",
   "metadata": {
    "id": "5f9833df"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "967fe297",
   "metadata": {
    "id": "967fe297"
   },
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bbcafc",
   "metadata": {
    "id": "f1bbcafc"
   },
   "source": [
    "1. Analytics Vidhya (2018). Natural Language Processing for Beginners: Using TextBlob. [online] Analytics Vidhya. Available at: https://www.analyticsvidhya.com/blog/2018/02/natural-language-processing-for-beginners-using-textblob/.\n",
    "\n",
    "2. Andriy Burkov (2019). THE HUNDRED-PAGE MACHINE LEARNING BOOK. Andriy Burkov\n",
    "\n",
    "3. Grus, J. (2015). Data Science from Scratch. ‘O’Reilly Media, Inc.’\n",
    "\n",
    "4. Hastie, T., Tibshirani, R. and Friedman, J.H. (2009). The Elements of Statistical Learning. Springer.\n",
    "\n",
    "5. IBM (2023). What is Random Forest? | IBM. [online] www.ibm.com. Available at: https://www.ibm.com/topics/random-forest.\n",
    "\n",
    "6. IBM (2024). AI vs. machine learning vs. deep learning vs. neural networks | IBM. [online] www.ibm.com. Available at: https://www.ibm.com/think/topics/ai-vs-machine-learning-vs-deep-learning-vs-neural-networks.\n",
    "\n",
    "7. Müller, A.C. and Guido, S. (2017). Introduction to machine learning with Python : a guide for data scientists. Beijing: O’reilly.\n",
    "\n",
    "8. Nitin Indurkhya and Damerau, F.J. (2010). Handbook of natural language processing. Boca Raton, Fl: Chapman & Hall/Crc.\n",
    "\n",
    "9. Oppermann, A. (2021). Activation Functions in Deep Learning: Sigmoid, tanh, ReLU. [online] KI Tutorials. Available at: https://artemoppermann.com/activation-functions-in-deep-learning-sigmoid-tanh-relu/#:~:text=Activation%20functions%20add%20a%20nonlinear [Accessed 30 May 2024].\n",
    "\n",
    "10. Provost, F. and Fawcett, T. (2013). Data science for business : what you need to know about data mining and data-analytic thinking. Sebastopol, CA: O’Reilly Media.\n",
    "\n",
    "11. Raschka, S. and Mirjalili, V. (2019). Python machine learning : machine learning and deep learning with python, scikit-learn, and tensorflow 2. Birmingham: Packt Publishing, Limited.\n",
    "\n",
    "\n",
    "### MODELS\n",
    "\n",
    "12. Chollet, F. (2020). The Sequential model. [online] keras.io. Available at: https://keras.io/guides/sequential_model/.\n",
    "\n",
    "13. scikit-learn.org. (n.d.). 3.2.4.3.2. sklearn.ensemble.RandomForestRegressor — scikit-learn 0.23.2 documentation. [online] Available at: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor.\n",
    "\n",
    "14. scikit-learn developers (2019). sklearn.linear_model.LinearRegression — scikit-learn 0.22 documentation. [online] Scikit-learn.org. Available at: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafaa8e1",
   "metadata": {
    "id": "dafaa8e1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab400c81",
   "metadata": {
    "id": "ab400c81"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feafd55",
   "metadata": {
    "id": "5feafd55"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec2ff61",
   "metadata": {
    "id": "aec2ff61"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2234f2",
   "metadata": {
    "id": "3b2234f2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
